[strings]
# Mode : train, test

mode = train

#Provide full path for preprocess files
train_enc = preprocessing/train.enc
train_dec = preprocessing/train.dec
test_enc = preprocessing/test.enc
test_dec = preprocessing/test.dec

# absolute path of the folder where checkpoints, vocabulary, temporary data will be stored
checkpoints = checkpoints/

#absolute path of folder for storing the tokenized data after data wrangling.
wrangling_directory = datawrangler/

[ints]
# vocabulary size 
enc_vocab_size = 20000
dec_vocab_size = 20000

#number of LSTM layers
num_layers = 3

# typical options : 128, 256, 512, 1024
layer_size = 256

# dataset size limit; typically none : no limit
max_train_data_size = 0

#
batch_size = 64

# steps per checkpoint
steps_per_checkpoint = 300

[floats]
#
learning_rate = 0.5

#
learning_rate_decay_factor = 0.99

#
max_gradient_norm = 5.0
